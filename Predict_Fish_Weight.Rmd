---
title: "Weight of Fish"
author: "Brennen Long"
date: "4/12/2021"
output: 
  html_document:
    number_sections: true
    toc: true
---


```{r include=FALSE,echo=FALSE}
require(tigerstats)
require(tidyverse)
require(car)
Allfish <- read.csv(file = "Fish.csv")
Allfish$Species <- as.factor(Allfish$Species)
levels(Allfish$Species)
```

```{r}
Perch <- subset(Allfish, Species == "Perch")
Perch <- select(Perch, -Species)
Bream <- subset(Allfish, Species == "Bream")
Bream <- select(Bream, -Species)
Smelt <- subset(Allfish, Species == "Smelt")
Smelt <- select (Smelt, -Species )
```


#Perch Model

## Results

```{r}
library(leaps)
regsubsets.out <-
    regsubsets(Weight ~ .,
               data = Perch,
               nbest = 1,       # 1 best model for each number of predictors
               nvmax = NULL,    # NULL for no limit on number of variables
               force.in = NULL, force.out = NULL,
               method = "exhaustive")
summary.out <- summary(regsubsets.out)
as.data.frame(summary.out$outmat)
library(car)
subsets(regsubsets.out,statistic="adjr2",legend="bottom",main="Adjusted R^2")
```

According to the automation, the best 2 variables for predicting Weight are Height and Width, because (according to the graphic) the Height- Width model has a greater Adjusted R-square value than all the other ones, at ~.94, or 94%.  

```{r}
which.max(summary.out$adjr2)
```

The model with those 2 variables has the greatest Adjusted R-squared, meaning it explains the greatest percentage of the data in Perch.  
  
```{r}
best.model <- lm(Weight~ Height + Width,data=Perch)
summary(best.model)
```

An Adjusted R-squared value of 0.94 means 94% of the length values can be explained by this model, which is incredibly useful (especially considering such a low p-value).  

## Inferential Results.

```{r}
fullup <- lm(Weight~ .,data=Perch)
summary.lm(fullup)
```

Height and Width have the lowest p-values, making them the most important variables. This model (including every variable) also has a slightly lower Adjusted R-squared value, making it less useful to us than the 2-variable model used earlier.  


# Bream model.  

## Results

```{r}
library(leaps)
regsubsets.out <-
    regsubsets(Weight ~ .,
               data = Bream,
               nbest = 1,       # 1 best model for each number of predictors
               nvmax = NULL,    # NULL for no limit on number of variables
               force.in = NULL, force.out = NULL,
               method = "exhaustive")
summary.out <- summary(regsubsets.out)
as.data.frame(summary.out$outmat)
library(car)
subsets(regsubsets.out,statistic="adjr2",legend="bottom",main="Adjusted R^2")
```

The best model for predicting a Bream's weight has 3 variables: length1, height, and width. While the L2-W model is probably the best 2-variable model, this must be tested manually to reduce chances of error.  

### Manual results

```{r}
fullup <- lm(Weight~ .,data=Bream)
summary.lm(fullup)
```

Height and Width seem to be the most important variables, which brings into question why L2-H was considered the best 2-variable model earlier. Length1 is the least important due to having the greatest p-value, so it'll be removed first.  
  
```{r}
MODL4 <- lm(Weight~ .-Length1,data=Bream)
summary.lm(MODL4)
```

The residual standard error decreased, and the Adjusted R-squared value increased from 0.9335 to 0.9355; this is indicative of a more accurate model. Length2 used to have the 2nd- highest p-value (higher than Length3's), but now Length3 has a higher p-value and thus will be removed next.  

```{r}
MODL3 <- lm(Weight~ .-Length1-Length3,data=Bream)
summary.lm(MODL3)
```

Standard error decreased while Adjusted R-squared increased, which is good. The listed p-values help to explain why L2-H was considered the best model: Width now has the highest p-value of 0.1873, so it will be removed next.  

```{r}
MODL2 <- lm(Weight~ .-Length1-Length3-Width,data=Bream)
summary.lm(MODL2)
```

This model is actually worse than the previous one, with an increased SE and a decreased Adj. R-squared. MODL2 (AKA. L2-H) will now be compared with a Width and Height model.  

```{r}
MODL0 <- lm(Weight~ Width + Height,data=Bream)
summary.lm(MODL0)
```

This is worse than the MODL2, due to a greater standard error and smaller Adj R-squared.  

## Inferential Results.

```{r}
anova(MODL2,fullup)
anova(MODL0, fullup)
anova(MODL0,MODL2, fullup)

```

The total sum of squares ("Sum of Sq" above) subtracts the variation accounted for by the model, but Residual Sum of Squares ("RSS") only takes into account the error variations. MODL0 has the greatest RSS and is thus the most error- prone; fullup (using every available variable) is the least error- prone. Normally, a higher sum of squares means there's more variability from the mean value. However, when comparing 2 models in anova, it has an additional meaning: MODL0 and MODL2 are the closest to each other, while MODL0 and fullup are the least similar. This is because sum of squares can be calculated as the difference (from subtraction) between the RSS values of each model. Using the first comparison as an example: (RSS of MODL2) - (RSS of fullup) = Sum of Squares, when taking into account rounding errors. However, fairly high p-values (greater than 0.4) suggest there is no significant difference between fullup and MODL2, and brings into question the difference in effectiveness between fullup and MODL0. Assuming the difference (in effectiveness) between a 2- variable and all- variable model is insignificant, the model with fewer variables should be chosen.  

```{r}
SumofSquares = 90244 - 84438
print(SumofSquares)
```

Thus, while fullup seems to be the best model out of the three (at least when using RSS to compare), MODL2 is confirmed to be the best 2-variable model.  

# Smelt model. 

## Results
```{r}
library(leaps)
regsubsets.out <-
    regsubsets(Weight ~ .,
               data = Smelt,
               nbest = 1,       # 1 best model for each number of predictors
               nvmax = NULL,    # NULL for no limit on number of variables
               force.in = NULL, force.out = NULL,
               method = "exhaustive")
summary.out <- summary(regsubsets.out)
as.data.frame(summary.out$outmat)
library(car)
subsets(regsubsets.out,statistic="adjr2",legend="bottomright",main="Adjusted R^2")
```

According to the graphic, L2-W is the best 2- variable model. To double- check this, L2-W will be compared with L2-H, H-W, and fullup.  

### Manual results.  

```{r}
fullup <- lm(Weight~ .,data=Smelt)
summary.lm(fullup)
```

This model uses every numerical variable in the dataset (other than weight) to predict the weight of a smelt. Length3 is the least important variable (with the highest p-value of 0.6643), which is why it isn't in the automated L1-L2-H-W model.  

```{r}
MODL01 <- lm(Weight~ Length2 + Width,data=Smelt)
summary.lm(MODL01)
```

This model's RSE value, as predicted by the graphic, is slightly higher than in the fullup model but much lower than in the H-W model. The Adjusted R-squared value, 0.9645, is only very slightly lower than in the model that used every variable.  

```{r}
MODL00 <- lm(Weight~ Height + Width,data=Smelt)
summary.lm(MODL00)
```

This model, H-W, has the highest RSE (0.9555) and a slightly lower Adjusted R-squared than both the fullup and L2-W model. Therefore, while fullup is the best model of the three analyzed, L2-W is the best 2- variable model.  

## Inferential Results.

```{r}
anova(MODL00, fullup)
anova(MODL01, fullup)
```

MODL00, or H-W, has the greatest Residual Sum of Squares, suggesting it is the most error- prone. The first comparison, between H-W and fullup, has a lower p- value than the second comparison, between L2-W and fullup. The null hypothesis is that, given there is no significant difference in effectiveness between the compared models, the data will be as is. Since the p-value is much higher in the second comparison, L2-W is much closer (in effectiveness) to fullup than H-W is, which aligns with the findings from earlier. However, since H-W also has a p-value over 0.05, the results are non-significant and the (best) model with the fewest variables should be chosen by default, making L2-W the most effective model.  